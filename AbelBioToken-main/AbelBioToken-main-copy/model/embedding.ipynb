{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add necessary directories to sys.path\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/data')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/train')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding is used to convert words (or token) into vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    In transformer, it will lose the positional information when processing\n",
    "    words in input sentences in parallel. Thus positional embedding is indispensable.\n",
    "\n",
    "    The final output embedding is $embedding + positional embedding$.\n",
    "    If change the position of words, the positional embedding would not changed, therefore\n",
    "    these two sentences will create different embedding sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len: int, embed_model_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "\n",
    "        # pe = torch.zeros(max_seq_len, embed_model_dim)\n",
    "        # position = torch.arange(0, max_seq_len, dtype=torch.double).unsqueeze(1)\n",
    "        # div_term = torch.exp(\n",
    "        #     torch.arange(0, embed_model_dim, 2, dtype=torch.double)\n",
    "        #     * (-math.log(1e4 / embed_model_dim))\n",
    "        # )\n",
    "        # pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / self.embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(\n",
    "                    pos / (10000 ** ((2 * (i + 1)) / self.embed_dim))\n",
    "                )\n",
    "\n",
    "        # [10, 512] -> [1, 10, 512]\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register buffer means the parameters in model, which should be saved and\n",
    "        # restored in the state_dict, but not trained by the optimizer.\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        # prevents the calculation of gradients for positional embedding\n",
    "        # during forward propagation\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        # print(\"pe shapeï¼š \", self.pe.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, max_seq_length: int, embed_dim: int) -> None:\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "\n",
    "        self.word_embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(max_seq_length, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.word_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        return self.dropout(x)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
