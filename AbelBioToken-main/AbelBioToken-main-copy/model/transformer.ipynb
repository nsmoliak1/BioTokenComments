{"cells":[{"cell_type":"code","execution_count":8,"id":"1398d10f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1398d10f","executionInfo":{"status":"ok","timestamp":1727356404767,"user_tz":-60,"elapsed":2118,"user":{"displayName":"nick smoliak","userId":"15020696766623238147"}},"outputId":"5faf6781-470b-4a91-c7d6-9a11997aa4da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n","from google.colab import drive\n","import sys\n","drive.mount('/content/drive')\n","\n","# Add necessary directories to sys.path\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/data')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/train')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/test')\n"]},{"cell_type":"code","execution_count":14,"id":"d31f3517","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"d31f3517","executionInfo":{"status":"error","timestamp":1727356680247,"user_tz":-60,"elapsed":380,"user":{"displayName":"nick smoliak","userId":"15020696766623238147"}},"outputId":"b4bf7b1d-7990-4ea4-e254-084bbc727dea"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"attempted relative import with no known parent package","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5752e7a65baf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model/encoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","from torch import nn\n","from . import encoder\n","from . import decoder\n","\n","print(torch.__version__)\n","\n","\n","class Transformer(nn.Module):\n","    \"\"\"\n","    Transformer model\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim,\n","        src_vocab_size,\n","        target_vocab_size,\n","        src_seq_length,\n","        trg_seq_length,\n","        num_layers=2,\n","        expansion_factor=4,\n","        n_heads=8,\n","    ):\n","        \"\"\"\n","        Initialize Transformer\n","\n","        Args:\n","            embed_dim: the dimension of embedding vectors\n","            src_vocab_size: the size of source vocabulary\n","            target_vocab_size: the size of target vocabulary\n","            src_seq_length: the length of sequence of source\n","            trg_seq_length: the length of sequence of target\n","            num_layers: the number of layers\n","            expansion_factor: factor which determines number of linear layers in feed forward layer\n","            n_heads: number of heads in multihead attetion\n","        \"\"\"\n","        super(Transformer, self).__init__()\n","\n","        self.target_vocab_size = target_vocab_size\n","        self.encoder = encoder.TransformerEncoder(\n","            src_seq_length,\n","            src_vocab_size,\n","            embed_dim,\n","            num_layers,\n","            expansion_factor,\n","            n_heads,\n","        )\n","        self.decoder = decoder.TransformerDecoder(\n","            target_vocab_size,\n","            embed_dim,\n","            trg_seq_length,\n","            num_layers,\n","            expansion_factor,\n","            n_heads,\n","        )\n","\n","    # TODO: adjust the length\n","    def transform(self, src: torch.Tensor, trg: torch.Tensor):\n","        enc_out = self.encoder(src)\n","        out_labels = []\n","        seq_len = src.shape[1]\n","        out = trg\n","        for _ in range(seq_len):\n","            out = self.decoder(out, enc_out)  # bs x seq_len x vocab+dim\n","            # taking the last token\n","            out = out[:, -1, :]\n","\n","            out = out.argmax(-1)\n","            out_labels.append(out.item())\n","            out = torch.unsqueeze(out, dim=0)\n","\n","        return out_labels\n","\n","    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> torch.Tensor:\n","        enc_out = self.encoder(src)\n","\n","        outputs = self.decoder(trg, enc_out)\n","        return outputs\n"]},{"cell_type":"code","source":[],"metadata":{"id":"7s-A8okicJvJ"},"id":"7s-A8okicJvJ","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}