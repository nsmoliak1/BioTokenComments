{"cells":[{"cell_type":"code","execution_count":null,"id":"b4f11ac6","metadata":{"id":"b4f11ac6"},"outputs":[],"source":["\n","from google.colab import drive\n","import sys\n","drive.mount('/content/drive')\n","\n","# Add necessary directories to sys.path\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/data')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/train')\n","sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/test')\n"]},{"cell_type":"code","execution_count":1,"id":"f12d58d9","metadata":{"id":"f12d58d9","executionInfo":{"status":"error","timestamp":1727356721442,"user_tz":-60,"elapsed":1,"user":{"displayName":"nick smoliak","userId":"15020696766623238147"}},"outputId":"5c8995e8-07c7-4808-a106-92dee60c2d50","colab":{"base_uri":"https://localhost:8080/","height":349}},"outputs":[{"ename":"ImportError","evalue":"attempted relative import with no known parent package","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3fe95886a800>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","from torch import nn\n","from . import embedding\n","from . import block\n","\n","\n","class TransformerEncoder(nn.Module):\n","    \"\"\"\n","    The encoder part of transformer. Responsible for encoding the input information.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        seq_len: int,\n","        vocab_size: int,\n","        embed_dim: int,\n","        num_layrs: int = 2,\n","        expansion_factor: int = 4,\n","        n_heads: int = 8,\n","    ) -> None:\n","        \"\"\"\n","        Initializes the TransformerEncoder\n","\n","        Args:\n","            seq_len: length of input sequence\n","            embed_dim: dimension of embedding\n","            num_layers: number of encoder layers\n","            expansion_factor: factor which determines number of linear layers in feed forward layer\n","            n_heads: number of heads in multihead attetion\n","        \"\"\"\n","        super(TransformerEncoder, self).__init__()\n","        self.transformer_embedding = embedding.TransformerEmbedding(\n","            vocab_size, seq_len, embed_dim\n","        )\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                block.TransformerBlock(embed_dim, expansion_factor, n_heads)\n","                for _ in range(num_layrs)\n","            ]\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Performs the forward pass for the transformer block.\n","\n","        Args:\n","            x: input of encoder\n","\n","        Returns:\n","            torch.Tensor: output of the encoder\n","        \"\"\"\n","        out = self.transformer_embedding(x)\n","        for layer in self.layers:\n","            out = layer(out, out, out)\n","\n","        return out\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}