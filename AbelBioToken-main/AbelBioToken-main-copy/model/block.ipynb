{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7454918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add necessary directories to sys.path\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/data')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/train')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple parapllel attention mechanism, adapting self-attention\n",
    "    and cross-attention in same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int = 512, n_heads: int = 8) -> None:\n",
    "        \"\"\"\n",
    "        Initialize multi-head attention.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim  # 512 dim\n",
    "        self.n_heads = n_heads  # 8\n",
    "        # 512/8=64, each key, query, value will be of 64\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)\n",
    "\n",
    "        # key, query and value matrixes, shape is 64 x 64\n",
    "        # Each head has different key, query and value matrixes.\n",
    "        self.query_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.key_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        # keep bias in output linear could enhence the flexi\n",
    "        self.out = nn.Linear(self.n_heads * self.single_head_dim, self.embed_dim)\n",
    "\n",
    "    def __get_attention(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        # (32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = (32 x 8 x 10 x 10)\n",
    "        product = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        product /= math.sqrt(self.single_head_dim)\n",
    "\n",
    "        return torch.matmul(F.softmax(product, dim=-1), value)\n",
    "\n",
    "    def __project(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "\n",
    "        # query dimension can change in decoder during inference.\n",
    "        # so we can't take general seq_length\n",
    "        seq_length_query = query.size(1)\n",
    "\n",
    "        q: torch.Tensor = self.query_matrix(query)\n",
    "        k: torch.Tensor = self.key_matrix(key)\n",
    "        v: torch.Tensor = self.value_matrix(value)\n",
    "\n",
    "        # 32x10x512\n",
    "        # batch_size x sequence_length x n_heads x single_head_dim = (32 x 10 x 8 x 64)\n",
    "        # -> batch_size x n_heads x sequence_length x single_head_dim = (32 x 8 x 10 x 64)\n",
    "        q = q.view(\n",
    "            batch_size, seq_length_query, self.n_heads, self.single_head_dim\n",
    "        ).transpose(1, 2)\n",
    "        k = k.view(\n",
    "            batch_size, seq_length, self.n_heads, self.single_head_dim\n",
    "        ).transpose(1, 2)\n",
    "        v = v.view(\n",
    "            batch_size, seq_length, self.n_heads, self.single_head_dim\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:  # batch_size x sequence_length x embedding_dim, 32 x 10 x 512\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: query vector\n",
    "            key: key vector\n",
    "            value: value vector\n",
    "            mask: mask for decoder\n",
    "        Returns:\n",
    "            torch.Tensor: vector from multihead attention\n",
    "        \"\"\"\n",
    "\n",
    "        # project the vectors to n_heads lower dimension spaces\n",
    "        # 32 x 10 x 512 -> 32 x 10 x 8 x 64\n",
    "        q, k, v = self.__project(query, key, value)\n",
    "\n",
    "        # acquire attention through softmax(QK^T/\\sqrt{d_k})V\n",
    "        attention = self.__get_attention(q, k, v, mask)\n",
    "\n",
    "        # concatenated output\n",
    "        # (32 x 8 x 10 x 64) -> (32 x 10 x 8 x 64) -> (32,10,512)\n",
    "        concat = (\n",
    "            attention.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(key.size(0), query.size(1), self.single_head_dim * self.n_heads)\n",
    "        )\n",
    "\n",
    "        return self.out(concat)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TransformerBlock represents a single block of the transformer encoder.\n",
    "\n",
    "    The overall process of one block is: input -> multi-head self attention -> add & norm -> feed forward -> add & norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim: int, expansion_factor: int = 4, n_heads: int = 8\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the TransformerBlock\n",
    "\n",
    "\n",
    "        Args:\n",
    "            embed_dim: dimension of the embedding\n",
    "            expansion_factor: factor which determines output dimension of linear layer\n",
    "            n_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "\n",
    "        self.attention_norm = nn.LayerNorm(embed_dim)\n",
    "        self.feed_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # FFN(x) = max(0, xW_1 + b_1) W_2 + b_2\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion_factor * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion_factor * embed_dim, embed_dim),\n",
    "        )\n",
    "        # enhence the generalization ability\n",
    "        self.attention_dropout = nn.Dropout(0.2)\n",
    "        self.feed_dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(\n",
    "        self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Performs the forward pass for the transformer block.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            query: query vector\n",
    "            key: key vector\n",
    "            value: value vector\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output of transformer block\n",
    "        \"\"\"\n",
    "        attention_out = self.attention(query, key, value)\n",
    "\n",
    "        # 32 x 10 x 512\n",
    "        attention_norm_out = self.attention_dropout(\n",
    "            self.attention_norm(attention_out + query)\n",
    "        )\n",
    "\n",
    "        # 32 x 10 x 512 -> 32 x 10 x 2048 -> 32 x 10 x 512\n",
    "        feed_fwd_out = self.feed_forward(attention_norm_out)\n",
    "\n",
    "        return self.feed_dropout(self.feed_norm(feed_fwd_out + attention_norm_out))\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    DecoderBlock include a multi-head self attention and a TransformerBlock (but use cross-attention).\n",
    "    self-attention -> dropout -> TransformerBlock (cross-attention)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embed_dim: int, expansion_factor: int = 4, n_heads: int = 8\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           query: query vector from decoder\n",
    "           key: key vector from encoder\n",
    "           value: value vector from encoder\n",
    "           mask: mask to be given for multi head attention\n",
    "\n",
    "        Returns:\n",
    "           torch.Tensor: output of transformer block\n",
    "        \"\"\"\n",
    "\n",
    "        # we need to pass mask only to fst attention\n",
    "        attention = self.attention(query, query, query, mask=mask)\n",
    "        query = self.dropout(self.norm(attention + query))\n",
    "\n",
    "        return self.transformer_block(query, key, value)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
