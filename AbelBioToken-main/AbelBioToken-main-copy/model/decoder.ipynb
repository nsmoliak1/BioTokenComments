{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add necessary directories to sys.path\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/data')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/model')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/train')\n",
    "sys.path.append('/content/drive/My Drive/AbelBioToken-main/AbelBioToken-main/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b515cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from . import block\n",
    "from . import embedding\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder part of transformer. Responsible for encoding the input information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_vocab_size,\n",
    "        embed_dim,\n",
    "        seq_len,\n",
    "        num_layers=2,\n",
    "        expansion_factor=4,\n",
    "        n_heads=8,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialze TransformerDecoder\n",
    "\n",
    "        Args:\n",
    "            target_vocab_size: the vocabulary size of target code\n",
    "            embed_dim: the embedding dimension\n",
    "            seq_len: the max sequnce length of target\n",
    "            num_layers: number of decoder layers\n",
    "            expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "            n_heads: number of heads in multihead attetion\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.transformer_embedding = embedding.TransformerEmbedding(\n",
    "            target_vocab_size, seq_len, embed_dim\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                block.DecoderBlock(embed_dim, expansion_factor, n_heads)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "\n",
    "    def make_target_mask(self, target: torch.Tensor):\n",
    "\n",
    "        batch_size, target_len, _ = target.shape\n",
    "        # return the lower triangular part of matrix filled with ones\n",
    "        target_mask = torch.tril(torch.ones((target_len, target_len))).expand(\n",
    "            batch_size, 1, target_len, target_len\n",
    "        )\n",
    "\n",
    "        return target_mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, enc_out: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "\n",
    "        Return:\n",
    "            torch.Tensor: output of decoder\n",
    "        \"\"\"\n",
    "        x = self.transformer_embedding(x)\n",
    "        mask = self.make_target_mask(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, mask)\n",
    "\n",
    "        return F.softmax(self.fc_out(x), dim=-1)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
